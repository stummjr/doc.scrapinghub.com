Scrapinghub Documentation
=========================

Scrapinghub is the most advanced platform for deploying and running web crawlers (also known as *spiders* or *scrapers*). It allows you to build crawlers easily, deploy them instantly and scale them on demand, without having to manage servers, backups or cron jobs. Everything is stored in a highly available database and retrievable using an API.

Spiders can be written in `Scrapy`_ or built using the `Autoscraping`_ tool, and they are grouped into projects. Each spider run is known as a *job*.

Here you will find reference documentation. For more articles, guides and other help resources please visit our `Knowledge Base`_ in our `Support center`_.

Table of Contents
-----------------

.. toctree::
   :maxdepth: 2

   plans
   dash
   shub
   scrapy-cloud
   api
   jobs
   jobdata
   comments
   portia
   reports
   activity
   collections
   frontier
   crawlera
   addons
   oldapi

.. _Support center: http://support.scrapinghub.com
.. _Knowledge Base: http://support.scrapinghub.com/forum/24895-knowledge-base/
.. _Scrapy: http://scrapy.org
.. _Autoscraping: http://scrapinghub.com/autoscraping
